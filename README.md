# **Unlocking the Potential of Arabic NLP: High-Quality Dataset and Preprocessing Framework for Arabic Large Language Models**

This project aims to build a **high-quality Arabic dataset for low-resource LLMs** focused on **academic content** while providing a **web-based preprocessing framework** called **Nukhba Labs**. The system **collects, cleans, annotates, fine-tunes, and evaluates** Arabic text for training **Arabic Large Language Models (LLMs)** with minimal manual effort.

---

### ğŸ“Œ **Project Status**
- âœ… Data collection, cleaning, and annotation completed  
- âœ… Initial LLM fine-tuning using AraBERT  
- âœ… Web-based framework deployed (Firebase + Flask)  
- âœ… Evaluation using grammar, lexical, readability, and topic metrics  
- â³ Final LLM integration in progress  

---

## ğŸ” **End-to-End Pipeline**

1. **Data Collection**
   - Source: Common Crawl, Arabic Datasets  
   - ğŸ”§ Script: `Download_WARC/download_warc.py`

2. **Data Cleaning**
   - Includes: Tokenization, HTML Cleaning, Deduplication, Blocklist Filtering  
   - ğŸ”§ Script: `CommonCrawl_Pipeline/pipeline.py`

3. **Annotation & Scoring**
   - Scored from 0â€“5 for academic relevance using **ALLaM-7B-Instruct**  
   - ğŸ”§ Script: `Testing_LLM/test.py`

4. **Model Fine-Tuning**
   - Annotated subset used to fine-tune **AraBERT**, which then auto-scores the full dataset  
   - ğŸ”§ Script: `Academic_Specific/Academic.py`

5. **Evaluation**
   - Evaluates processed datasets using:
     - âœ… Grammar scoring (LLM-based)
     - âœ… Lexical diversity metrics
     - âœ… Readability analysis
     - âœ… Topic distribution via BERTopic  
   - ğŸ”§ Script: `Evaluation/Evaluate.py`  
   - ğŸ§  SLURM Jobs: `run_evaluate.sh`

6. **LLM Integration**
   - Final step: Fine-tune or integrate academic dataset into a large Arabic LLM  
   - ğŸš§ In progress

---

## ğŸŒ **Nukhba Labs Web Framework**

A full-stack Arabic dataset preprocessing framework.

### ğŸ”‘ **Key Features**
- Firebase Auth (email, Google, GitHub)
- Drag & drop dataset upload
- Selectable cleaning steps (tokenize, normalize, remove noise, etc.)
- Output format selector (CSV, JSON, XLSX)
- Dataset history and download
- Backend powered by Flask (Render)

### ğŸ“ Web Files
| Page | Function |
|------|----------|
| `index.html` | Landing page with login + explainer video |
| `login.html` | Secure login (email + social auth) |
| `signup.html` | Signup linked to Firebase |
| `dashboard.html` | View history of cleaned datasets |
| `upload.html` | Upload interface with cleaning options |
| `preview.html` | Download confirmation screen |
| `firebase.js` | Firebase Auth + Firestore config |
| `app.py` | Flask backend for cleaning |
| `render.yaml` | Render deployment config |
| `requirements.txt` | Python backend dependencies |

---

## ğŸ› ï¸ **Technologies Used**

- **Transformers** â€“ ALLaM-7B-Instruct, AraBERT, LLaMA-3 via Hugging Face  
- **DeepSpeed + SLURM** â€“ HPC model training  
- **Firebase Auth** â€“ Google, GitHub, Email login  
- **Flask** â€“ Python backend for file processing  
- **BERTopic** â€“ Topic modeling on academic data  
- **TextStat, NLTK** â€“ Readability & lexical metrics  
- **HTML/CSS/JS** â€“ Full frontend UI  
- **Render** â€“ Deploy backend API  
- **GitHub Pages** â€“ Deploy frontend  


---

## ğŸ”— Live Demo

- **Frontend:** [https://ameeraattiah.github.io/nukhba-labs](https://ameeraattiah.github.io/nukhba-labs)  
- **Backend:** Deployed via Render 


