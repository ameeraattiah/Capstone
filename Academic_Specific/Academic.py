## Import Libraries
import json
from sklearn.metrics import f1_score
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, AutoModelForSequenceClassification
import torch
from transformers import T5Tokenizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
from sentencepiece import SentencePieceProcessor
import os
from transformers import AutoModelForCausalLM
from transformers import GenerationConfig
import sys
import re
import wandb
import deepspeed
import os
import numpy
try:
    import mpi4py
    from mpi4py import MPI
except ImportError:
    print("MPI is not available, proceeding without it.")
    MPI = None
from sklearn.model_selection import train_test_split
from collections import Counter


print("SentencePiece is installed and ready to use.")

device = "cuda" if torch.cuda.is_available() else "cpu"
torch.cuda.empty_cache()  # Clears unused GPU memory
print(f"Using device: {device}")

# Load DeepSpeed configuration from JSON file
deepspeed_config_path = "/ibex/user/abuhanjt/test/deepspeed_config.json"
with open(deepspeed_config_path, "r") as ds_file:
    deepspeed_config = json.load(ds_file)

config = {
    "model_name": "meta-llama/Llama-3.1-8B-Instruct",  # Mistral for annotation
    "fine_tune_model": "aubmindlab/bert-base-arabertv02",  # AraBERT for classification
    "threshold": 2,  # Minimum score for high-quality content
    "annotation_samples": 5000,  # Increase annotated samples for better learning
    "validation_samples": 1000,  # More validation samples for better evaluation
    "max_samples_to_fine_tune": 4000,  # More fine-tuning samples
    "epochs": 5,  
    "batch_size": 1, 
    "learning_rate": 1e-5,  # Adjust learning rate for large model
    "gradient_accumulation_steps": 8,  # Helps handle large models with small batch sizes
    "use_deepspeed": True,  # Enable DeepSpeed for efficient training
    "dataset_dir": "/ibex/user/abuhanjt/test/dataset/",  
    "output_dir": "/ibex/user/abuhanjt/test/output/",
    "deepspeed_config_path": deepspeed_config_path
}

# Initialize Weights & Biasis
wandb.init(
    project="Academic_Specific",  
    name="experiment_250", 
    config=config 
)

# Ensure output directory exists
os.makedirs(config["output_dir"], exist_ok=True)

## Arabic Rubric Prompt
def get_arabic_prompt(text):
    return f"""
    <|system|>
    ูููุง ููู ููุชุทู ูู ุตูุญุฉ ููุจ. ูู ุจุชูููู ูุฏู ูุงุฆุฏุชู ููุญุชูู ุชุนูููู ุจุงุณุชุฎุฏุงู ูุธุงู ุชูููู ูููู ูู 5 ููุงุท ุชุฑุงูููุฉุ ููููุง ูููุนุงููุฑ ุงูุชุงููุฉ:

    *ูุนุงููุฑ ุงูุชูููู ุงูุชุฑุงููู (ูุน ูุฑููุฉ ูู ุงูุชูุฏูุฑ ุจูุงุกู ุนูู ุฌูุฏุฉ ุงููุญุชูู(:
    โข	ุฃุถู 1 ููุทุฉ: ุฅุฐุง ูุงู ุงููุต ูุญุชูู ุนูู ุจุนุถ ุงููุนูููุงุช ุงูุฃุณุงุณูุฉ ุงููููุฏุฉุ ุญุชู ูู ูุงู ูุชุถูู ูุญุชูู ุบูุฑ ุชุนูููู ูุซู ุงูุฅุนูุงูุงุช ุฃู ุงูููุงุฏ ุงูุชุฑููุฌูุฉุ ูููู ูุง ูุณุงูู ูู ุงูุชุนูู ุจุดูู ูุงุถุญ.
    โข	ุฃุถู 2 ููุงุท: ุฅุฐุง ูุงู ุงููุต ูุชูุงูู ุนูุงุตุฑ ุฐุงุช ุตูุฉ ุจุงูุชุนูููุ ูููู ูุง ููุชุฒู ุชูุงููุง ุจุงููุนุงููุฑ ุงูุฃูุงุฏูููุฉุ ููุฏ ูููู ููู ูุฒูุฌ ูู ุงููุนูููุงุช ุงููููุฏุฉ ูุบูุฑ ุงููููุฏุฉ ุฃู ููุฏู ูุธุฑุฉ ุนุงูุฉ ุฏูู ุชูุงุตูู ูุงููุฉ.
    โข	ุฃุถู 3 ููุงุท: ุฅุฐุง ูุงู ุงููุต ููุงุณุจูุง ููุงุณุชุฎุฏุงู ุงูุชุนููููุ ูุญุชูู ุนูู ููุงููู ุฑุฆูุณูุฉ ุฐุงุช ุตูุฉ ุจุงูููุงูุฌ ุงููุฏุฑุณูุฉุ ููู ูุงุถุญุ ูููู ูุฏ ููุชูุฑ ุฅูู ุจุนุถ ุงูุชูุงุตูู ุฃู ูุญุชูู ุนูู ูุนูููุงุช ุฅุถุงููุฉ ููุณุช ุถุฑูุฑูุฉ.
    โข	ุฃุถู 4 ููุงุท: ุฅุฐุง ูุงู ุงููุต ููุธููุงุ ูุงุถุญูุงุ ููุชูุงุณููุงุ ููููู ุงุณุชุฎุฏุงูู ููุฑุฌุน ุชุนูููู ุฌูุฏ. ูุดุจู ูุตููุง ุฏุฑุงุณููุง ูู ูุชุงุจ ูุฏุฑุณูุ ูุญุชูู ุนูู ุฃูุซูุฉ ูุชูุงุฑููุ ูููุฏู ูุนูููุงุช ุฏูููุฉุ ุญุชู ูู ูุงู ููุงู ุจุนุถ ุงูุชูุงุตูู ุงูููููุฏุฉ.
    โข	ุฃุถู 5 ููุงุท: ุฅุฐุง ูุงู ุงููุต ุนุงูู ุงูุฌูุฏุฉ ุชุนูููููุงุ ูููู ุงุณุชุฎุฏุงูู ูุจุงุดุฑุฉ ูู ุงูุชุฏุฑูุณุ ูุญุชูู ุนูู ูุญุชูู ุดุงููุ ููุธู ุจุงููุงููุ ุจุดุฑูุญุงุช ูุงุถุญุฉ ูุฃูุซูุฉ ูุชูุงุฑููุ ุฏูู ุฃู ูุนูููุงุช ุบูุฑ ุถุฑูุฑูุฉ ุฃู ุบููุถ. ูููู ุฃู ูููู ูุซุงูููุง ูุฌุฒุก ูู ูุงุฏุฉ ุชุนููููุฉ ุฑุณููุฉ.

    ุฅุฐุง ูุงู ุงููุต ูุจุฏู ุชุนูููููุง ููููู ูุง ูุญุตู ุนูู ุชูููู ูุฑุชูุนุ ูู ุจูุฑุงุนุงุฉ ุฃุณููุจ ุชูุฏูู ุงููุนูููุงุช ููุฏู ูุงุฆุฏุชูุง ุงููุนููุฉ ููุทุงูุจุ ุจุฏูุงู ูู ุงูุงุนุชูุงุฏ ููุท ุนูู ุงูุชูุงู ูู ุนูุตุฑ.

    * ุงููุต ุงููุทููุจ ุชููููู:
    {text}

    *ุงูุชูููู ุงูููุงุฆู:
    ูุฑุฌู ุชูุฏูู ุฅุฌุงุจุฉ ุฏูููุฉ ููุจุงุดุฑุฉ ููููุง ูููุนุงููุฑ ุฃุนูุงู:
    1.	ูุฏู ุชุจุฑูุฑูุง ูุงุถุญูุง ููุจุงุดุฑูุง ููุถุญ ุณุจุจ ุงุฎุชูุงุฑู ููุฐู ุงูุฏุฑุฌุฉ ุจูุงุกู ุนูู ูุญุชูู ุงููุต ูููุณ ููุท ูุนุงููุฑ ุงูุชูููู ุงูุนุงูุฉ.
    2.	ุงุณุชุฎุฏู ุงูุชูุณูู ุงูุชุงูู ูููุชูุฌุฉ ุงูููุงุฆูุฉ: ( ุงูุชูููู ุงูุชุนูููู : <ูุฌููุน ุงูููุงุท>/ 5)

    <|assistant|>
    ุงุณุชูุงุฏูุง ุฅูู ุงููุนุงููุฑุ ุงูุชูููู ูู:
    """



## Custom Dataset Class
from torch.utils.data import Dataset
class CustomDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        item = {key: val.squeeze(0) for key, val in encoding.items()}
        item["labels"] = torch.tensor(label, dtype=torch.long)
        return item
    

## Step 1: Load Dataset
def load_dataset(dataset_file):

    with open(dataset_file, "r", encoding="utf-8") as file:
        data = json.load(file)

    cleaned_data = []
    for item in data:
        # Ensure 'text' exists and provide default values for missing metadata
        if "text" in item:
            cleaned_data.append({
                "text": item["text"],
                "metadata": {
                    "timestamp": item.get("timestamp", "N/A"), 
                    "url": item.get("url", "N/A"),
                    "source": item.get("source", "Unknown") 
                }
            })
        else:
            print(f"Warning: Skipping entry in {dataset_file} - Missing 'text' field")

    return cleaned_data

# Function to clean and separate score and evaluation
def extract_score_and_evaluation(output):
    #  Locate evaluation section first
    evaluation_match = re.search(r"(ุงุณุชูุงุฏูุง ุฅูู ุงููุนุงููุฑุ ุงูุชูููู ูู:.*)", output, re.DOTALL)
    evaluation_text = evaluation_match.group(1).strip() if evaluation_match else "ูุง ููุฌุฏ ุชูููู ูุชุงุญ."

    #  Find score **inside** the evaluation text
    score_match = re.search(r"ุงูุชูููู ุงูุชุนูููู\s*:\s*\(*\s*(\d+)(?:\.\d+)?\s*/\s*5\s*\)*", evaluation_text)

    if score_match:
        score = int(score_match.group(1))  # Ensure score is an integer
    else:
        score = 0  #  If score is missing, default to 0 instead of None

    return score, evaluation_text


## Step 2: Annotate Data Locally and save it 
# Get SLURM Task ID  
slurm_task_id = os.getenv("SLURM_ARRAY_TASK_ID", "0")
# Define unique save path per SLURM task
ANNOTATION_SAVE_PATH = f"/ibex/user/abuhanjt/test/output/annotations_{slurm_task_id}.json"
print(f"Annotations will be saved to: {ANNOTATION_SAVE_PATH}")

# Ensure the annotation file exists (creates an empty JSON file if missing)
if not os.path.exists(ANNOTATION_SAVE_PATH):
    with open(ANNOTATION_SAVE_PATH, "w", encoding="utf-8") as file:
        json.dump([], file, ensure_ascii=False, indent=4)  # Initialize empty list
    print(f"Created new annotation file: {ANNOTATION_SAVE_PATH}")

def annotate_samples(samples, tokenizer, model):
    annotated_data = []

    # **Check if there's a previous backup and load it safely**
    if os.path.exists(ANNOTATION_SAVE_PATH):
        try:
            with open(ANNOTATION_SAVE_PATH, "r", encoding="utf-8") as file:
                annotated_data = json.load(file)
            
            # Ensure `annotated_data` is a **list**
            if not isinstance(annotated_data, list):
                print("Warning: Annotation file is invalid. Resetting annotations.")
                annotated_data = []

            # Stop if we already reached 5000 annotations
            if len(annotated_data) >= 5000:
                print(f"Already 5,000 annotations completed. Skipping annotation.")
                return annotated_data
            else:
                print(f"Resuming annotation from {len(annotated_data)} samples.")

        except (json.JSONDecodeError, FileNotFoundError):
            print("Warning: Annotation file was corrupted. Resetting annotations.")
            annotated_data = []

    # Avoid duplicate processing
    processed_ids = {entry["id"] for entry in annotated_data}

    for idx, sample in enumerate(samples, start=1):
        if idx in processed_ids:
            continue  # Skip already annotated samples

        text = sample["text"]
        arabic_prompt = get_arabic_prompt(text)

        inputs = tokenizer(
            arabic_prompt,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=4000
        ).to(device)

        inputs["attention_mask"] = (inputs["input_ids"] != tokenizer.pad_token_id).to(device)

        generation_config = GenerationConfig(
            max_new_tokens=100,
            do_sample=True,
            temperature=0.5,  
            top_k=40,
            top_p=0.85,
            repetition_penalty=1.25 
        )

        torch.cuda.empty_cache()

        raw_output = tokenizer.decode(
            model.generate(
                inputs["input_ids"], 
                attention_mask=inputs["attention_mask"], 
                generation_config=generation_config
            )[0], 
            skip_special_tokens=True
        )

        score, evaluation = extract_score_and_evaluation(raw_output)

        annotated_sample = {
            "id": idx,  # Unique document ID
            "text": text,
            "scores": score,
            "metadata": sample["metadata"],
            "evaluation": evaluation  
        }

        annotated_data.append(annotated_sample)

        # Save progress after every sample
        with open(ANNOTATION_SAVE_PATH, "w", encoding="utf-8") as file:
            json.dump(annotated_data, file, ensure_ascii=False, indent=4)

        print(f"๐น Saved annotation {idx}/{len(samples)}")

        # Log to W&B
        wandb.log({
            "sample_id": idx,
            "text": text,
            "predicted_score": score,
            "evaluation_summary": evaluation[:400],
        })

    print(f"Annotation complete. Total samples annotated: {len(annotated_data)}")
    return annotated_data



## Step 3: Fine-Tune AraBERT
def fine_tune_arabert(train_data, val_data, tokenizer, model):
    torch.cuda.empty_cache()

    texts_train = [item["text"] for item in train_data]
    labels_train = [item["scores"] for item in train_data]

    texts_val = [item["text"] for item in val_data]
    labels_val = [item["scores"] for item in val_data]

    train_dataset = CustomDataset(texts_train, labels_train, tokenizer)
    val_dataset = CustomDataset(texts_val, labels_val, tokenizer)

    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=config["epochs"],
        per_device_train_batch_size=config["batch_size"],
        evaluation_strategy="epoch",
        save_strategy="epoch",
        logging_dir="./logs",
        logging_steps=100,
        gradient_accumulation_steps=config["gradient_accumulation_steps"],
        learning_rate=config["learning_rate"], 
        warmup_steps=500, 
        weight_decay=0.01, 
        max_grad_norm=1.0, 
        deepspeed=config["deepspeed_config_path"],
        lr_scheduler_type="cosine",  # Choose from ['linear', 'cosine', 'constant', etc.]
        bf16=True,
        report_to="wandb"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )

    print(f"Starting fine-tuning... (Train: {len(train_data)}, Val: {len(val_data)})")
    trainer.train()
    print("Fine-tuning complete.")



## Step 4: Predict with Fine-Tuned AraBERT     
def predict_with_arabert(unlabeled_data, model, tokenizer):
    model.eval()
    model = deepspeed.init_inference(model, dtype=torch.bfloat16)  # Add DeepSpeed for inference

    model.to(device)
    predictions = []

    for sample in unlabeled_data:
        text = sample["text"]
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {key: val.to(device) for key, val in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
        logits = outputs.logits
        predicted_label = logits.argmax(dim=-1).item()

        predictions.append({
            "text": text,
            "predicted_score": predicted_label,
            "metadata": sample["metadata"]
        })

    return predictions


## Step 5: Filter Dataset
def filter_dataset(annotated_data, threshold):
    return [
        doc for doc in annotated_data
        if doc.get("predicted_score", 0) >= threshold
    ]


## The Main Pipeline
def main_pipeline():
    # Ensure a file is provided
    if len(sys.argv) < 2:
        raise ValueError("No dataset file provided!")

    # Step 1: Load the dataset from a specified JSON file
    dataset_args = [arg for arg in sys.argv if not arg.startswith("--")]
    if len(dataset_args) < 2:
        raise ValueError("No dataset file provided!")
    dataset_file = dataset_args[1] 

    print(f"Processing file: {dataset_file}")
    dataset = load_dataset(dataset_file)
    print(f"Loaded {len(dataset)} samples.")

    # Step 2: Select a subset of the dataset for annotation
    sample_data = dataset[:config["annotation_samples"]]

    # Load tokenizer for Mistral-7B (text generation)
    tokenizer_mistral = AutoTokenizer.from_pretrained(config["model_name"], trust_remote_code=True)
    tokenizer_mistral.pad_token = tokenizer_mistral.eos_token  

    # Load model for annotation (Mistral)
    model_mistral = AutoModelForCausalLM.from_pretrained(
        config["model_name"],
        torch_dtype=torch.float16,  # FP16 to reduce memory usage
        device_map="auto",  
        trust_remote_code=True  
    )

    # Annotate data using Mistral
    annotated_data = annotate_samples(sample_data, tokenizer_mistral, model_mistral)
    print(f"Annotated {len(annotated_data)} samples.")

    # Step 3: Load tokenizer and model for fine-tuning AraBERT
    tokenizer_arabert = AutoTokenizer.from_pretrained(config["fine_tune_model"])
    tokenizer_arabert.pad_token = tokenizer_arabert.eos_token if tokenizer_arabert.eos_token else "[PAD]"

    model_arabert = AutoModelForSequenceClassification.from_pretrained(
        config["fine_tune_model"], 
        num_labels=6
    ).to(device)

    # Filter out invalid scores first
    filtered_data = [item for item in annotated_data if 0 <= item["scores"] <= 5]

    # Ensure dataset has sufficient samples
    if len(filtered_data) < config["max_samples_to_fine_tune"] + config["validation_samples"]:
        raise ValueError(f"Not enough data for training & validation. Found {len(filtered_data)} samples.")

    # Extract labels for stratified splitting
    labels = [item["scores"] for item in filtered_data]

    # Stratified train-validation split (ensures class balance)
    train_data, val_data = train_test_split(
        filtered_data,
        test_size=config["validation_samples"] / len(filtered_data),
        stratify=labels,  # Maintains class proportions
        random_state=42  # Ensures reproducibility
    )

    # Check new label distributions
    train_label_counts = Counter([item["scores"] for item in train_data])
    val_label_counts = Counter([item["scores"] for item in val_data])

    print(f"Training samples: {len(train_data)} | Validation samples: {len(val_data)}")
    print("New Training Label Distribution:", train_label_counts)
    print("New Validation Label Distribution:", val_label_counts)

    # Step 5: Fine-tune the model using the annotated data
    fine_tune_arabert(train_data, val_data, tokenizer_arabert, model_arabert)

    # Step 6: Use the fine-tuned model to predict the remaining dataset
    remaining_data = dataset[config["annotation_samples"]:]
    predictions = predict_with_arabert(remaining_data, model_arabert, tokenizer_arabert)
    print(f"Predicted {len(predictions)} samples with fine-tuned AraBERT.")

    # Step 7: Filter the predictions to include only high-quality samples
    filtered_data = filter_dataset(predictions, config["threshold"])
    print(f"Filtered dataset contains {len(filtered_data)} high-quality samples.")

    # Step 8: Save processed data
    output_file = os.path.join(config["output_dir"], f"processed_{os.path.basename(dataset_file)}")
    with open(output_file, "w", encoding="utf-8") as file:
        json.dump(filtered_data, file, ensure_ascii=False, indent=4)
    print(f"Saved processed data to {output_file}")

if __name__ == "__main__":
    main_pipeline()
    print("Running academic dataset processing...")

wandb.finish()
